{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 1\n\n# Verify SparkContext\nprint(sc)\n\n# Print Spark version\nprint(sc.version)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 2\n\n# Import SparkSession from pyspark.sql\nfrom pyspark.sql import SparkSession\n\n# Create my_spark\nmy_spark = SparkSession.builder.getOrCreate()\n\n# Print my_spark\nprint(my_spark)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 3\n\n# Print the tables in the catalog\nprint(spark.catalog.listTables())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 4\n\n# Don't change this query\nquery = \"FROM flights SELECT * LIMIT 10\"\n\n# Get the first 10 rows of flights\nflights10 = spark.sql(query)\n\n# Show the results\nflights10.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 5\n\n# Don't change this query\nquery = \"SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest\"\n\n# Run the query\nflight_counts = spark.sql(query)\n\n# Convert the results to a pandas DataFrame\npd_counts = flight_counts.toPandas()\n\n# Print the head of pd_counts\nprint(pd_counts.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 6\n\n# Don't change this query\nquery = \"SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest\"\n\n# Run the query\nflight_counts = spark.sql(query)\n\n# Convert the results to a pandas DataFrame\npd_counts = flight_counts.toPandas()\n\n# Print the head of pd_counts\nprint(pd_counts.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 7\n\n# Create pd_temp\npd_temp = pd.DataFrame(np.random.random(10))\n\n# Create spark_temp from pd_temp\nspark_temp = spark.createDataFrame(pd_temp)\n\n# Examine the tables in the catalog\nprint(spark.catalog.listTables())\n\n# Add spark_temp to the catalog\nspark_temp.createOrReplaceTempView(\"temp\")\n\n# Examine the tables in the catalog again\nprint(spark.catalog.listTables())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exercise 8\n\n# Don't change this file path\nfile_path = \"/usr/local/share/datasets/airports.csv\"\n\n# Read in the airports data\nairports = spark.read.csv(file_path,header=True)\n\n# Show the data\nairports.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}